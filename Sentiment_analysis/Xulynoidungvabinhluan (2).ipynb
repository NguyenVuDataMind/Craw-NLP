{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sử dụng Sentence Embedding để xây dựng mô hình phân loại cảm xúc\n",
    "Sentence Embedding là cách biểu diễn văn bản dưới dạng vector sử dụng các mô hình nhúng ngữ nghĩa sâu (như Sentence-BERT, FastText, hoặc Transformers).Ở đây sử dụng Sentence-BERT (SBERT), một mô hình dựa trên BERT nhưng được tối ưu hóa để tính toán tương đồng và vector hóa câu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 1: Load dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mẫu train: 30000\n",
      "Số mẫu test: 10000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Đọc dữ liệu từ folder (CÁ NHÂN HÓA ĐƯỜNG DẪN `base_dir` DƯỚI ĐÂY)\n",
    "def load_data_from_folder(base_dir):\n",
    "    \"\"\"\n",
    "    base_dir: Thư mục gốc chứa dữ liệu train/test với cấu trúc:\n",
    "    ├── test\n",
    "    │   ├── neg\n",
    "    │   └── pos\n",
    "    └── train\n",
    "        ├── neg\n",
    "        └── pos\n",
    "    \"\"\"\n",
    "    texts, labels = [], []\n",
    "    for label_type in [\"neg\", \"pos\"]:\n",
    "        dir_name = os.path.join(base_dir, label_type)\n",
    "        for fname in os.listdir(dir_name):\n",
    "            if fname.endswith(\".txt\"):\n",
    "                with open(os.path.join(dir_name, fname), encoding=\"utf-8\") as f:\n",
    "                    texts.append(f.read())\n",
    "                labels.append(0 if label_type == \"neg\" else 1)\n",
    "    return texts, labels\n",
    "\n",
    "train_dir = \"D:/Code/KPW/Final_project_Auto_craw_NLP/data_train/data_train/train\"\n",
    "test_dir = \"D:/Code/KPW/Final_project_Auto_craw_NLP/data_test/data_test/test\"\n",
    "\n",
    "train_texts, train_labels = load_data_from_folder(train_dir)\n",
    "test_texts, test_labels = load_data_from_folder(test_dir)\n",
    "\n",
    "print(f\"Số mẫu train: {len(train_texts)}\")\n",
    "print(f\"Số mẫu test: {len(test_texts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 2: Tiền xử lý"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mẫu dữ liệu sau tiền xử lý: ['mua có mỗi bingsu thập_cẩm k mà mình f đợi hơn hỏi lại thì nv tl có r nhg bảo chờ thêm nữa tụi e lm liền mình k biết có ngon k nhg cũng muốn ăn thử thiết_nghĩ nv quán nên xem_lại cách pv và nc vs khách']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "# Hàm tiền xử lý văn bản\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    text: Văn bản cần xử lý\n",
    "    - Chuyển thành chữ thường\n",
    "    - Loại bỏ số, ký tự đặc biệt, khoảng trắng thừa\n",
    "    - Tách từ tiếng Việt\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return ViTokenizer.tokenize(text)\n",
    "\n",
    "# Áp dụng tiền xử lý cho dữ liệu\n",
    "train_texts = [preprocess_text(text) for text in train_texts]\n",
    "test_texts = [preprocess_text(text) for text in test_texts]\n",
    "\n",
    "print(f\"Mẫu dữ liệu sau tiền xử lý: {train_texts[:1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 3: Vector hóa với Sentence-BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Code\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ADMIN\\.cache\\huggingface\\hub\\models--sentence-transformers--distiluse-base-multilingual-cased-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Batches: 100%|██████████| 938/938 [19:13<00:00,  1.23s/it]\n",
      "Batches: 100%|██████████| 313/313 [07:29<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước vector của một câu: (512,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Tải mô hình Sentence-BERT\n",
    "model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")\n",
    "\n",
    "# Vector hóa dữ liệu\n",
    "X_train = model.encode(train_texts, show_progress_bar=True)\n",
    "X_test = model.encode(test_texts, show_progress_bar=True)\n",
    "\n",
    "print(f\"Kích thước vector của một câu: {X_train[0].shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 4: Huấn luyện mô hình\n",
    "Không cần cá nhân hóa, nhưng bạn có thể thay đổi siêu tham số max_iter nếu muốn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Độ chính xác trên tập test: 0.70\n",
      "Báo cáo phân loại:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.70      0.70      5000\n",
      "           1       0.70      0.69      0.69      5000\n",
      "\n",
      "    accuracy                           0.70     10000\n",
      "   macro avg       0.70      0.70      0.70     10000\n",
      "weighted avg       0.70      0.70      0.70     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Huấn luyện Logistic Regression\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train, train_labels)\n",
    "\n",
    "# Dự đoán trên tập test\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "print(f\"Độ chính xác trên tập test: {accuracy_score(test_labels, y_pred):.2f}\")\n",
    "print(\"Báo cáo phân loại:\")\n",
    "print(classification_report(test_labels, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 5: Lưu mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mô hình và Sentence-BERT đã được lưu.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "model_save_path = \"D:/Code/KPW/Final_project_Auto_craw_NLP/model_save_path/sentiment_classifier.pkl\"\n",
    "vectorizer_save_path = \"D:/Code/KPW/Final_project_Auto_craw_NLP/vectorizer_save_path/sentence_transformer.pkl\"\n",
    "\n",
    "# Lưu mô hình phân loại và Sentence-BERT\n",
    "os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "joblib.dump(clf, model_save_path)\n",
    "joblib.dump(model, vectorizer_save_path)\n",
    "print(\"Mô hình và Sentence-BERT đã được lưu.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bước 6: Dự đoán với dữ liệu mới\n",
    "Bạn có thể cá nhân hóa văn bản đầu vào new_text theo ngữ cảnh sử dụng của bạn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Code\\.venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Tải mô hình đã lưu\n",
    "loaded_clf = joblib.load(\"D:/Code/KPW/Final_project_Auto_craw_NLP/model_save_path/sentiment_classifier.pkl\")\n",
    "loaded_model = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyvi import ViTokenizer\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    text: Văn bản cần xử lý\n",
    "    - Chuyển thành chữ thường\n",
    "    - Loại bỏ số, ký tự đặc biệt, khoảng trắng thừa\n",
    "    - Tách từ tiếng Việt\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = text.strip()\n",
    "    return ViTokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dự đoán cảm xúc: Tiêu cực\n"
     ]
    }
   ],
   "source": [
    "new_text = \"Thầy Thủy tuy khó nhưng dạy hay\"\n",
    "preprocessed_text = preprocess_text(new_text)\n",
    "vectorized_text = loaded_model.encode([preprocessed_text])\n",
    "prediction = loaded_clf.predict(vectorized_text)\n",
    "\n",
    "print(f\"Dự đoán cảm xúc: {'Tích cực' if prediction[0] == 1 else 'Tiêu cực'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dự đoán cảm xúc: Tiêu cực\n"
     ]
    }
   ],
   "source": [
    "new_text = \"Thầy Thủy rất đẹp trai\"\n",
    "preprocessed_text = preprocess_text(new_text)\n",
    "vectorized_text = loaded_model.encode([preprocessed_text])\n",
    "prediction = loaded_clf.predict(vectorized_text)\n",
    "\n",
    "print(f\"Dự đoán cảm xúc: {'Tích cực' if prediction[0] == 1 else 'Tiêu cực'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bước 7: Thực hiện lấy dữ liệu từ MongoDB về và phân tích cảm xúc bình luận"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "Chuyện là hôm qua mình có \"vô tình\" truy cập vào trang web phim hành động ít diễn viên (trong này chắc chưa ông nào mà chưa  \"vô tình\" lần nào nhỉ \n",
      " ).\n",
      "Trong này có ông nào làm trong nhà mạng internet thì cho mình hỏi cái này: từ lâu tới nay mọi người vẫn nghĩ nhà mạng logs lại internet activities của clients, vậy với số lượng client và lượng truy cập khổng lồ thì cái database dùng để log nó phải có tốc độ xử lý khủng khiếp và dung lượng lưu trữ lớn tới như thế nào mới có thể đáp ứng được việc này.\n",
      "Hay thực sự không có logs nhỉ? \n",
      " #j2team_ask\n",
      "có cách nào tải về khi idm không bắt link được không mn nhỉ ?\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Các huynh đài cho hỏi, giờ dùng extension nào chặn q/c ytb vậy. Ublock hết dc r\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Câu hỏi hay đấy, họ đo traffic hàng tuần, tháng và năm để xem nhu cầu của người dùng. Họ tracking các ip truy cập đến các trang web 'ko bình thường' và add nó vào danh sách 'cần lưu ý'. Và khi có biến xảy ra thì thường họ sẽ track lại xem user đó có đang truy cập hay ko. Và việc còn lại là của conan điều tra. Việc bác xem ếch ko ai quan tâm nhé. Hi vọng sẽ giải đáp cho bạn\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Ông ơi xem sếch thì tập trung đi ông đừng học nữa mà\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "vào đó thì tập trung việc chính đi bác\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Đấy là HTTP ngày xưa thôi. Giờ HTTPS thì log kiểu gì.\n",
      "Dự đoán cảm xúc: Tích cực\n",
      ". lát em quay lại đọc ạ\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Trước em có xem 1 video kể về vụ 1 ông mở sàn tmđt giống như shopee trên deepweb kiếm đc hàng triệu $, Sau vô tình bị cảnh sát tìm ra cái email chính của ông ý quảng cáo trang web lúc mới xây dựng và bị tra ra danh tính, họ log lại lịch sử truy cập mạng của ông ý sau đó mới túm đc.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Theo đó thì nhà mạng họ không log lại hết toàn bộ traffic mà chỉ log với user nhất định khi có yêu cầu của công an.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Log trong một khung thời gian thôi, như khoảng 3 năm. Log củ xóa đi. Lưu log nói chung ko tốn kém nhiều vì append only mà.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "lấy tài sản của Elon Musk, đổi ra mệnh giá 500k của Vn rồi xếp lên mặt đất, 1 lớp.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "khi nào hình dung ra nó sẽ thành cái gì thì sẽ biết data lớn như nào.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Cá là ông này vừa xuất chắc luôn\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Tập trung chuyên môn đi ông anh\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "theo minh thi log minh chi ghi ra file hoac day len s3, sau nay minh can gi thi xu ly log de extract va filter data minh can day vao db thoi\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Để lưu trữ số lượng lớn thì chắc sẽ làm như cách của bên discord chẳng hạn =)))\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "thử xin việc vô mấy nhà mạng làm thử r về chia sẻ ae đi\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Traffic, session, device fingerprint bắt dưới modem, log ra file và gửi theo periodic, thường là 24h.\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Nghiên cứu\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Like em với để tối em đọc comment\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Chắc ko có đâu. Nếu ông để DNS của nhà mạng thì may ra mới log, còn lại nhà mạng chỉ cần quan tâm từng thời điểm ông có địa chỉ nào khi vào mạng để có yêu cầu tra soát từ cơ quan chức năng thì tra ra ông thôi :))\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Theo mình chắc log những trang hoặc những người cần theo dõi thôi\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "-------------------------------------------------------------------------------\n",
      "Em xin phép được đăng ẩn danh ạ, mong ad duyệt bài ạ\n",
      "em xin đc giới thiệu qua về bản thân, em là người khuyết tật, tay chân cũng chậm và nói cũng không rõ, em học bách khoa ngành kỹ thuật, tuy nhiên vì không xin vào được đúng chuyên ngành nên em đã nhận làm kế toán cho 1 công ty gia đình, và sau hơn 1 năm làm công việc kế toán em càng ngày càng chán công việc vì nhiều lý do, đăc biệt là nó rất nhàm chán.\n",
      "Dự định trong năm mới em quyết tâm xin vào các vị trí văn phòng liên quan đến kỹ thuật như RnD... nhưng hơn 1 năm làm kế toán nên CV của em vẫn chưa có gì. Em chấp nhận bắt đầu lại từ đầu nhưng khi pv em nói k rõ lại bị từ chối. mọi người cho em xin lời khuyên để cải thiện bản thân ạ\n",
      "Em cảm ơn mọi người ạ\n",
      "#j2team_question\n",
      "Never give up\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Em comment để hóng các ý kiến từ cao nhân khác.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "======\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Ở góc nhìn của em thì đúng là khá khó, chỉ có thể tăng năng lực để bù đắp cho giọng nói. Tuy nhiên thì tăng năng lực thì lại cần giọng nói ....\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "======\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Hoặc cần bỏ thời gian để luyện tập cách nói\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Chào bạn. IT của công ty mình cũng là NKT. Bạn có thể chia sẻ thông tin liên hệ để mình kết nối 2 người không?\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "bạn làm ngành gì ? RnD thì có vẻ là mấy ngành cơ bản như hoá học ?\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Hơi khó nhỉ\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Chào em anh cũng là cựu sinh viên Đại Học Bách Khoa Hà Nội cũng bị khuyệt tật như em như anh bị nói ngọng không ai nghe rõ viết bằng tay trái và hiện tại a đang làm Lập trình viên trên HÀ Nội tự nuôi sống bản thân mình á\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Đậu Việt có lẽ chỉ có cách đó, làm gì cũng cần phải giao tiếp với đồng nghiệp cả, kể cả làm sếp cũng phải giao tiếp để giao việc cho cấp dưới. Bạn này muốn tư vấn cũng ko muốn nói rõ là học ngành gì, tật như thế nào để mọi người tư vấn, ẩn danh rổi lại ẩn thêm thông tin.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Smallie Tran rõ ràng là vậy. Không có giao tiếp, kể cả có năng lực, nếu là em, em cũng khá phân vân trc khi chọn.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Vì đơn giản là em sẽ chọn dc ng tốt hơn với cùng năng lực.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "-------------------------------------------------------------------------------\n",
      "#j2team_share\n",
      "Một số hình ảnh được Grok - AI của X tạo thông qua dòng lệnh.\n",
      "Nhìn biết AI luôn, vì nó không có thần. Còn bạn không nhận ra AI thì bạn nên trả tiền cho AI\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "kbt mn thế nào nhưng mình nhìn phát biết AI, AI có kiểu vẽ rất đặc trưng í\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Một ngày nào đó liệu các MMO bros có tận dụng nó để tạo ra những trang cá nhân có life story và các bài đăng liên tục như một con người thật từ việc sử dụng AI để tạo ra hình anh? Tôi không biết. Nhưng thời điểm hiện tại rất nhiều các trang cá nhân đăng công khai hình ảnh bị các MMO bros clone nó ra như một song trùng tồn tại song song với người thật (đặc biệt là các bạn nữ thường xuyên đăng tải ảnh công khai). Xem đánh giá, cmt seeding trên các page còn không biết đâu là người thật đâu là clone nữa, thật đáng sợ!\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Sao AI nhìn nó cứ giả trân ntn ý\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Bạn không nhắc là AI tạo ra thì đa số cứ nghĩ là ảnh chụp thật.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "khéo mai sub nhầm onlyfan của AI\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Nhìn kỹ thì là AI rồi, xem xét các tiểu tiết thì sẽ rõ thôi. Phím đàn AI vẫn chưa tạo ra tốt.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "làm việc dùng AI nhiều, nên nhìn đâu cũng nghi ngờ\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Hiện tại nhìn vẫn biết đc là AI hay hông mà\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "nhìn real v\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Mình dùng Grok để gen hình xe hơi thì hơn Meta AI 1 trời 1 vực, Meta a đuồi kinh:))\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Đôi bàn tay vẫn là thứ cao siêu mà k con ai nào làm tốt dc\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Kêu ng thật cũng tin, sợ v\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "-------------------------------------------------------------------------------\n",
      "Android 16 support linux.\n",
      "Có bác nào thử code và build chưa?\n",
      "Đây là giải pháp chạy Vscode và build ngay trên androd 16.\n",
      "#j2team_share\n",
      "#j2team_ask\n",
      "gì cũng được miễn có VSCode trên điện thoại\n",
      "cầm laptop cũ tiếng kêu to như máy cày khổ lắm=))\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Code trên android thì cần mỗi cái terminal của termux là đủ rồi á bị cái tương tác với mấy cái api của android hơi chậm thôi, để sắp tới xem thử terminal gốc của google mượt hơn k\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "hiện tại đã có thể dùng linux và vscode từ lâu rồi bạn, andronix\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "GPU hiện chỉ có mesa3d tích hợp cho Adreno thôi\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "-------------------------------------------------------------------------------\n",
      "Mạng Viettel bỗng dưng bị chặn khỏi một vài trang của Microsoft, nguyên nhân do đâu và liệu có cách khắc phục mà không cần dùng VPN?\n",
      "Không biết giữa Microsoft và Viettel đang có vấn đề gì nhưng bữa giờ rất nhiều người bị chặn khi vào trang Microsoft trong đó có cả mình, cho dù có bật data trên điện thoại thì vào vẫn không được.\n",
      "Muốn biết có bị chặn hay không thì các bạn chỉ cần vào trang tải windows 10 dưới đây hoặc tự tìm với từ khoá \"windows 10 download\" trên google:\n",
      "https://www.microsoft.com/en-us/software-download/windows10\n",
      "Khi vào trang, nếu bị chặn bạn sẽ thấy độ trễ cao và các icon không load đầy đủ, không thể ấn được nút nào cả. Chỉ cần reload trang là bị hiện dòng chữ \"Access Denied\" ngay lập tức. Trình trạng này cũng xảy ra với trang tải windows 11 luôn.\n",
      "ㅤ\n",
      "Mặc dù có thể dùng các cách vượt như VPN, trang proxysite hoặc các công cụ bên thứ 3 để tải file mà không cần vào trang, nhưng đó chỉ là dành cho những ai biết thôi. Còn với những người không rành thì rất thiệt thòi cho họ khi phải tải các trang trôi nổi trên mạng, chưa chắc đã là phiên bản windows chuẩn, cài dễ phát sinh lỗi.\n",
      "ㅤ\n",
      "Có bác nào có cao kiến gì có thể tự fix được mà không cần sử dụng các công cụ khác không hay đành phải đợi nhà mạng tự fix?\n",
      "#j2team_ask\n",
      "Say bye viettel r :))\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Tui bị hoài, cách nhanh nhất là gọi tổng đài, sau đó ktv vào kiểm tra tiến hành định tuyến lại là ko bị chặn\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "vô bình thường mà\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "\"không cần dùng tới công cụ nào\" thì đổi sang VNPT thôi :))))))\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "GoodBye DPI chắc được\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Cay Viettel này thật, cứ 8h tối là bóp ngẹt đăng thông quốc tế\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Cái này bị lâu r mà. Phải tầm từ tháng 10 t cài lại win đã thấy bị r. Trước cứ nghĩ bị đứt cáp hay gì đấy\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Cấm Steam xong qua microsoft à\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "a Bill lạ lắm, trước lâu lâu cài lại win k vào đc trang chủ để tải mấy cái linh tinh nữa cơ\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Xưa tôn thờ Viettel vì ổn định\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Giờ như cái đầu ..\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Vịt teo quay tít. Đồng nghiệp của mình cũng dùng Vịt teo thỉnh thoảng bị tình trạng không phân giải DNS các trang crypto, phải dùng DNS của CloudFlare hoặc NextDNS mới vào được.\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Mình cũng bị giống bạn khi vào bằng Edge. Đổi qua Chrome thì vào bt\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Cả njav nữa …\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "thế mà t tưởng do wifi nhà trọ, lên công ty vào phà phà.\n",
      "Dự đoán cảm xúc: Tích cực\n",
      ":)) Hồi trc t cũng bị ấy, vào mấy trang của dell vĩng bị chặn. Hỏi mấy ông nhà mạng thì kêu có vào cái gì linh tinh k. Bộ vào web cấm mà t mang đi hỏi hay gì.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Mình vô bình thường mà\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "fake vpn\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Viettel mạng tệ nhất trong các nhà mạng mình sử dụng\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Bị hổm nay chắc hơn tuần rồi. Ko bít sao mạng khác thì dùng bình thường mỗi viettel là bị.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Cả zvideoz nữa\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Họ đang test\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Cũng hên xui lắm, mở chrome này ko đc nhưng chrome khác lại đc\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "bth mà nhỉ\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "bị chặn lâu lắm rồi luôn í =(((\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "ồ, mình cũng bị y hệt\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Mình viettel, bị vậy miết luôn, thậm chí acc MD còn bị temporary lock vì lưu lượng truy cập. Bị sao ấy\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "bye viettel là vừa\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Mạng a7 dạo này bất ổn vch kiểu này qua meta thôi\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Đức Chung tưởng lại bay acc\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Thiên Phú nhớ mấy tháng trước e nói block mà a k tin\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "-------------------------------------------------------------------------------\n",
      "Top  quán nướng bơ nên thử ở Đà Nẵng\n",
      "- Nướng bơ 1986 : 319 Nguyễn Tri Phương, Đà Nẵng\n",
      "Ê thịt tươi lắm mấy bà, mà cắt cục nào cục nấy to nên ăn đã lắm.\n",
      "Nướng bơ mini 35 lê quý đôn cũng ngon lắm\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Phải chỗ em dắt đi Kim Duyên\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Đinh Thị Hồng Nhung đi ăn đi ăn\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "bữa em nói quán ni á Hương Thanh\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "quán có nướng than ko\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Thủy Tiên Ánh Tuyết thông điệp hả ta\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "siêu ngon bổ rẻ luôn\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Thơm Nguyen Cục dàng có thích mấy món này hông? Em dẫn đi ăn :)))\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Minh chân áii\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Rcm mng quán này nha\n",
      "toẹt dời lắm ạ\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "ngonnnn thiệt nha mn\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Khôi Thân ăn nướng cái kiểu này ngon hơn chảo gang\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Rv thật lòng: ngon, rẻ\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Lý Vĩ nào đi lại\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Rốt Nguyễn lolo\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Dương Quỳnh thèm đồ nướng\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Hải Yến ừ hứ\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "..\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "-------------------------------------------------------------------------------\n",
      " Tiệm ốp la bò né yêu thik của mình đã quay trở lại \n",
      ". Bò né ở đây lát to, dày, mềm. Một phần ốp la bò né 40k mà ăn no nức nở nào bò, trứng, pate kèm ổ bmi nữa. Còn sức ăn lớn thì gọi phần đặc biệt 55k với 2 trứng, bò né, xíu mại, xúc xích, chả mỡ, pate. Xíu mại, pate thơm béo, bò mềm, chả và xúc xích giòn cùng nước sốt đặc biệt thấm chấm bmi hết bài \n",
      " Pepper - 137 Nguyễn Chí Thanh\n",
      " 6h30 ~ 10h30\n",
      " Menu và giá pic cuối\n",
      "CHUYÊN: TH.U M.U.A XE MÁY CŨ GI.Á CAO TẬN NƠI ĐÀ NẴNG.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "SĐT : 0935.535.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "CẢM ƠN ĐÃ QUAN TÂM.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Ăn sáng thôi mn ơi\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Triet Huynh Quang sáng mai ăn\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Hic... Món này ăn thử mấy quán toàn bò đông lạnh.\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Giờ làm ăn k có tâm\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Văn Nam Tú\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Triet Huynh Quang\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Mai Hoang\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Ngô Thôi\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Tiên Thủy đi mi\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Rất ngon\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Nguyễn Ngọc Hoài An lên kèo\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Nhiều bò xĩu, ngon quá\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Chỉ 40k với bò né ngon quá đi\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Ngon rất ngon\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Ngon đã đời\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "ngon lắm shop\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Quá trời\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Đin Đin\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "-------------------------------------------------------------------------------\n",
      "Thời tiết se se lạnh như thế này thì đi ăn bắp nướng, khoai nướng đi mn uiii \n",
      " 230 Núi Thành, ĐN\n",
      "Trần Tiến Dũng ấm\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Cho mình hỏi bảng giá ạ\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Nga Cyn chillaaaaa\n",
      "Dự đoán cảm xúc: Tích cực\n",
      "Hoài Lê thèm chưaaa\n",
      "Dự đoán cảm xúc: Tiêu cực\n",
      "Lâm Anh Khoa noted\n",
      "Dự đoán cảm xúc: Tiêu cực\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('mongodb+srv://MLadmin:admin1021@machinelearning.so3qxxp.mongodb.net/')\n",
    "db = client['KPW']\n",
    "col_group = db['groupScraping']\n",
    "col_fanpage = db['fanpageScraping']\n",
    "d=0\n",
    "\n",
    "type = input('Bạn muốn phân tích cảm xúc trong bình luận của group hay fanpage? (group/fanpage): ')\n",
    "\n",
    "if type == 'fanpage':\n",
    "    for item in col_fanpage.find():\n",
    "        for i in item['Cmts']:\n",
    "            if d==0:\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print(item['Text-Content'])\n",
    "            d+=1\n",
    "            \n",
    "            preprocessed_text = preprocess_text(i)\n",
    "            vectorized_text = loaded_model.encode([preprocessed_text])\n",
    "            prediction = loaded_clf.predict(vectorized_text)\n",
    "            print(i)\n",
    "            print(f\"Dự đoán cảm xúc: {'Tích cực' if prediction[0] == 1 else 'Tiêu cực'}\")\n",
    "        d=0\n",
    "else:\n",
    "    for item in col_group.find():\n",
    "        for i in item['Cmts']:\n",
    "            if d==0:\n",
    "                print('-------------------------------------------------------------------------------')\n",
    "                print(item['Text-Content'])\n",
    "            d+=1\n",
    "            \n",
    "            preprocessed_text = preprocess_text(i)\n",
    "            vectorized_text = loaded_model.encode([preprocessed_text])\n",
    "            prediction = loaded_clf.predict(vectorized_text)\n",
    "            print(i)\n",
    "            print(f\"Dự đoán cảm xúc: {'Tích cực' if prediction[0] == 1 else 'Tiêu cực'}\")\n",
    "        d=0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
